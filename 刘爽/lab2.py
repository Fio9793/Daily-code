import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import chardet
from scipy import stats
from sklearn.impute import SimpleImputer
import warnings

warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

# å®šä¹‰Type 2åˆ—çš„æ— å…³å€¼ï¼ˆéœ€è¦åˆ é™¤çš„å†…å®¹ï¼‰
INVALID_TYPE2_VALUES = {'A', 'BBB', '0', 'a', 'bbb', '0.0'}  # åŒ…å«å¤§å°å†™å’Œå¯èƒ½çš„æ•°å€¼å½¢å¼


# æ£€æµ‹æ–‡ä»¶ç¼–ç 
def detect_encoding(file_path):
    with open(file_path, 'rb') as f:
        result = chardet.detect(f.read())
    return result['encoding']


# è¯»å–æ•°æ®
def load_pokemon_data(file_path='Pokemon.csv'):
    try:
        # å…ˆå°è¯•è‡ªåŠ¨æ£€æµ‹ç¼–ç 
        encoding = detect_encoding(file_path)
        print(f"æ£€æµ‹åˆ°çš„æ–‡ä»¶ç¼–ç : {encoding}")

        # å°è¯•ä¸åŒçš„ç¼–ç 
        encodings_to_try = [encoding, 'utf-8', 'latin-1', 'cp1252', 'iso-8859-1']

        for enc in encodings_to_try:
            try:
                print(f"å°è¯•ä½¿ç”¨ {enc} ç¼–ç è¯»å–æ–‡ä»¶...")
                df = pd.read_csv(file_path, encoding=enc)
                print(f"æˆåŠŸä½¿ç”¨ {enc} ç¼–ç è¯»å–æ–‡ä»¶ï¼Œåˆå§‹è§„æ¨¡ï¼š{df.shape}")
                return df
            except UnicodeDecodeError:
                continue
            except Exception as e:
                print(f"ä½¿ç”¨ {enc} ç¼–ç æ—¶å‡ºé”™: {e}")
                continue

        # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œä½¿ç”¨é”™è¯¯å¤„ç†æ–¹å¼
        print("æ‰€æœ‰ç¼–ç å°è¯•å¤±è´¥ï¼Œä½¿ç”¨é”™è¯¯å¤„ç†æ–¹å¼è¯»å–...")
        df = pd.read_csv(file_path, encoding='utf-8', errors='replace')
        return df

    except FileNotFoundError:
        print("æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„")
        return None
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return None


# æ•°æ®æ¦‚è§ˆ
def data_overview(df):
    print("=" * 50)
    print("æ•°æ®æ¦‚è§ˆ")
    print("=" * 50)
    print(f"æ•°æ®é›†å½¢çŠ¶: {df.shape}")
    print("\nå‰5è¡Œæ•°æ®:")
    print(df.head())

    print("\næ•°æ®åŸºæœ¬ä¿¡æ¯:")
    print(df.info())

    print("\næè¿°æ€§ç»Ÿè®¡:")
    print(df.describe())

    print("\nç¼ºå¤±å€¼ç»Ÿè®¡:")
    missing_data = df.isnull().sum()
    print(missing_data[missing_data > 0])

    return df


# æ•°æ®ç±»å‹æ£€æŸ¥
def check_data_types(df):
    print("\n" + "=" * 50)
    print("æ•°æ®ç±»å‹æ£€æŸ¥")
    print("=" * 50)

    # æ£€æŸ¥åˆ—åï¼Œå¤„ç†å¯èƒ½çš„ç¼–ç é—®é¢˜
    print("åˆ—å:", df.columns.tolist())

    numeric_columns = df.select_dtypes(include=[np.number]).columns
    categorical_columns = df.select_dtypes(include=['object']).columns

    print(f"æ•°å€¼å‹åˆ—: {list(numeric_columns)}")
    print(f"åˆ†ç±»å‹åˆ—: {list(categorical_columns)}")

    return numeric_columns, categorical_columns


# é‡å¤å€¼æ£€æµ‹
def detect_duplicates(df):
    print("\n" + "=" * 50)
    print("é‡å¤å€¼æ£€æµ‹")
    print("=" * 50)

    # å®Œå…¨é‡å¤çš„è¡Œ
    duplicate_rows = df[df.duplicated(keep=False)]
    if not duplicate_rows.empty:
        print(f"å‘ç° {len(duplicate_rows)} è¡Œå®Œå…¨é‡å¤çš„æ•°æ®:")
        # è·å–ç¬¬ä¸€åˆ—å’Œåç§°åˆ—çš„åˆ—å
        first_col = df.columns[0]
        name_col = 'Name' if 'Name' in df.columns else df.columns[1] if len(df.columns) > 1 else df.columns[0]
        print(duplicate_rows[[first_col, name_col]].head(10))
    else:
        print("æ²¡æœ‰å‘ç°å®Œå…¨é‡å¤çš„è¡Œ")

    # åç§°é‡å¤çš„å®å¯æ¢¦
    name_col = 'Name' if 'Name' in df.columns else df.columns[1] if len(df.columns) > 1 else None
    if name_col:
        name_duplicates = df[df.duplicated(name_col, keep=False)]
        if not name_duplicates.empty:
            print(f"\nå‘ç° {len(name_duplicates)} è¡Œåç§°é‡å¤çš„å®å¯æ¢¦:")
            first_col = df.columns[0]
            print(name_duplicates[[first_col, name_col]].head(10))

    return duplicate_rows, name_duplicates


# å¼‚å¸¸å€¼æ£€æµ‹ - æ•°å€¼å‹æ•°æ®
def detect_numeric_outliers(df, numeric_columns):
    print("\n" + "=" * 50)
    print("æ•°å€¼å¼‚å¸¸å€¼æ£€æµ‹")
    print("=" * 50)

    outlier_info = {}

    for col in numeric_columns:
        if col in df.columns:
            # ç¡®ä¿åˆ—æ˜¯æ•°å€¼å‹
            df[col] = pd.to_numeric(df[col], errors='coerce')

            # ä½¿ç”¨IQRæ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR

            outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]

            if not outliers.empty:
                print(f"\n{col} åˆ—çš„å¼‚å¸¸å€¼ (IQRæ–¹æ³•):")
                print(f"æ­£å¸¸èŒƒå›´: [{lower_bound:.2f}, {upper_bound:.2f}]")
                print(f"å‘ç° {len(outliers)} ä¸ªå¼‚å¸¸å€¼")
                outlier_info[col] = outliers

                # æ˜¾ç¤ºéƒ¨åˆ†å¼‚å¸¸å€¼
                first_col = df.columns[0]
                name_col = 'Name' if 'Name' in df.columns else df.columns[1] if len(df.columns) > 1 else df.columns[0]
                outlier_samples = outliers[[first_col, name_col, col]].head(5)
                for _, row in outlier_samples.iterrows():
                    print(f"  #{row[first_col]} {row[name_col]}: {row[col]}")

    return outlier_info


# ç‰¹æ®Šå€¼æ£€æµ‹
def detect_special_values(df):
    print("\n" + "=" * 50)
    print("ç‰¹æ®Šå€¼æ£€æµ‹")
    print("=" * 50)

    special_cases = {}
    numeric_cols = ['Total', 'HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def', 'Speed']

    # æ£€æµ‹0æˆ–è´Ÿå€¼
    for col in numeric_cols:
        if col in df.columns:
            # ç¡®ä¿åˆ—æ˜¯æ•°å€¼å‹
            df[col] = pd.to_numeric(df[col], errors='coerce')
            zero_or_negative = df[df[col] <= 0]
            if not zero_or_negative.empty:
                print(f"\n{col} åˆ—ä¸­çš„0æˆ–è´Ÿå€¼:")
                first_col = df.columns[0]
                name_col = 'Name' if 'Name' in df.columns else df.columns[1] if len(df.columns) > 1 else df.columns[0]
                print(zero_or_negative[[first_col, name_col, col]].head())
                special_cases[f'{col}_zero_negative'] = zero_or_negative

    # æ£€æµ‹ç§‘å­¦è®¡æ•°æ³•è¡¨ç¤ºçš„å€¼
    for col in numeric_cols:
        if col in df.columns:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ç§‘å­¦è®¡æ•°æ³•å­—ç¬¦
            sci_notation = df[df[col].astype(str).str.contains('e|E', na=False)]
            if not sci_notation.empty:
                print(f"\n{col} åˆ—ä¸­çš„ç§‘å­¦è®¡æ•°æ³•æ•°å€¼:")
                first_col = df.columns[0]
                name_col = 'Name' if 'Name' in df.columns else df.columns[1] if len(df.columns) > 1 else df.columns[0]
                print(sci_notation[[first_col, name_col, col]].head())
                special_cases[f'{col}_scientific'] = sci_notation

    return special_cases


# é€»è¾‘ä¸€è‡´æ€§æ£€æŸ¥
def check_consistency(df):
    print("\n" + "=" * 50)
    print("é€»è¾‘ä¸€è‡´æ€§æ£€æŸ¥")
    print("=" * 50)

    issues = []

    # ç¡®ä¿æ•°å€¼åˆ—æ˜¯æ•°å€¼ç±»å‹
    stat_cols = ['HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def', 'Speed']
    for col in stat_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')

    # æ£€æŸ¥Totalåˆ—æ˜¯å¦ç­‰äºå„å±æ€§ä¹‹å’Œ
    if all(col in df.columns for col in ['Total', 'HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def', 'Speed']):
        calculated_total = df[['HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def', 'Speed']].sum(axis=1)
        total_mismatch = df[abs(df['Total'] - calculated_total) > 1]
        if not total_mismatch.empty:
            print("Totalåˆ—ä¸å„å±æ€§ä¹‹å’Œä¸åŒ¹é…:")
            first_col = df.columns[0]
            name_col = 'Name' if 'Name' in df.columns else df.columns[1] if len(df.columns) > 1 else df.columns[0]
            print(total_mismatch[[first_col, name_col, 'Total']].head())
            issues.append('total_mismatch')

    # æ£€æŸ¥Generationå’ŒLegendaryåˆ—æ˜¯å¦è¢«ç½®æ¢
    if 'Generation' in df.columns and 'Legendary' in df.columns:
        # æ£€æŸ¥Generationåˆ—ä¸­æ˜¯å¦æœ‰å¸ƒå°”å€¼
        gen_bool_like = df[df['Generation'].astype(str).str.upper().isin(['TRUE', 'FALSE'])]
        # æ£€æŸ¥Legendaryåˆ—ä¸­æ˜¯å¦æœ‰æ•°å­—
        leg_numeric = pd.to_numeric(df['Legendary'], errors='coerce').notna()
        legendary_numeric = df[leg_numeric]

        if not gen_bool_like.empty or not legendary_numeric.empty:
            print("\nå¯èƒ½å­˜åœ¨Generationå’ŒLegendaryåˆ—ç½®æ¢çš„æƒ…å†µ:")
            first_col = df.columns[0]
            name_col = 'Name' if 'Name' in df.columns else df.columns[1] if len(df.columns) > 1 else df.columns[0]
            if not gen_bool_like.empty:
                print("Generationåˆ—ä¸­çš„å¸ƒå°”å€¼:")
                print(gen_bool_like[[first_col, name_col, 'Generation', 'Legendary']].head())
            if not legendary_numeric.empty:
                print("Legendaryåˆ—ä¸­çš„æ•°å€¼:")
                print(legendary_numeric[[first_col, name_col, 'Generation', 'Legendary']].head())
            issues.append('column_swap')

    return issues


# å¯è§†åŒ–æ£€æµ‹
def visualize_anomalies(df):
    print("\n" + "=" * 50)
    print("å¯è§†åŒ–å¼‚å¸¸æ£€æµ‹")
    print("=" * 50)

    # è®¾ç½®å›¾å½¢
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('å®å¯æ¢¦æ•°æ®å¼‚å¸¸å€¼å¯è§†åŒ–æ£€æµ‹', fontsize=16, fontweight='bold')

    # 1. Attackå±æ€§çš„æ•£ç‚¹å›¾
    if 'Attack' in df.columns:
        df['Attack'] = pd.to_numeric(df['Attack'], errors='coerce')
        axes[0, 0].scatter(range(len(df)), df['Attack'], alpha=0.6, color='red')
        axes[0, 0].set_title('Attackå±æ€§åˆ†å¸ƒæ•£ç‚¹å›¾')
        axes[0, 0].set_xlabel('æ•°æ®ç´¢å¼•')
        axes[0, 0].set_ylabel('Attackå€¼')
        axes[0, 0].grid(True, alpha=0.3)

    # 2. æ•°å€¼å±æ€§çš„ç®±çº¿å›¾
    numeric_cols = ['HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def', 'Speed']
    available_numeric = []
    for col in numeric_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
            available_numeric.append(col)

    if available_numeric:
        df[available_numeric].boxplot(ax=axes[0, 1])
        axes[0, 1].set_title('ä¸»è¦æ•°å€¼å±æ€§ç®±çº¿å›¾')
        axes[0, 1].tick_params(axis='x', rotation=45)

    # 3. Totalå±æ€§çš„åˆ†å¸ƒ
    if 'Total' in df.columns:
        df['Total'] = pd.to_numeric(df['Total'], errors='coerce')
        axes[0, 2].hist(df['Total'].dropna(), bins=30, alpha=0.7, color='green')
        axes[0, 2].set_title('Totalå±æ€§åˆ†å¸ƒç›´æ–¹å›¾')
        axes[0, 2].set_xlabel('Totalå€¼')
        axes[0, 2].set_ylabel('é¢‘æ•°')

    # 4. Type 2ç¼ºå¤±æƒ…å†µ
    type2_col = None
    for col in df.columns:
        if 'Type' in col and '2' in col:
            type2_col = col
            break

    if type2_col:
        type2_missing = df[type2_col].isna().value_counts()
        labels = ['æœ‰ç¬¬äºŒç±»å‹', 'æ— ç¬¬äºŒç±»å‹'] if len(type2_missing) == 2 else ['æœ‰ç¬¬äºŒç±»å‹']
        axes[1, 0].pie(type2_missing.values, labels=labels, autopct='%1.1f%%')
        axes[1, 0].set_title('Type 2ç¼ºå¤±æƒ…å†µ')

    # 5. é‡å¤å€¼æ£€æµ‹çƒ­å›¾
    if len(available_numeric) > 1:
        correlation = df[available_numeric].corr()
        im = axes[1, 1].imshow(correlation, cmap='coolwarm', aspect='auto')
        axes[1, 1].set_title('æ•°å€¼å±æ€§ç›¸å…³æ€§çƒ­å›¾')
        axes[1, 1].set_xticks(range(len(available_numeric)))
        axes[1, 1].set_yticks(range(len(available_numeric)))
        axes[1, 1].set_xticklabels(available_numeric, rotation=45)
        axes[1, 1].set_yticklabels(available_numeric)
        plt.colorbar(im, ax=axes[1, 1])

    # 6. å¼‚å¸¸å€¼æ±‡æ€»
    outlier_counts = {}
    for col in available_numeric:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        outliers = ((df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))).sum()
        outlier_counts[col] = outliers

    axes[1, 2].bar(outlier_counts.keys(), outlier_counts.values(), color='orange')
    axes[1, 2].set_title('å„å±æ€§å¼‚å¸¸å€¼æ•°é‡')
    axes[1, 2].tick_params(axis='x', rotation=45)

    plt.tight_layout()
    plt.show()


# æ¸…æ´—å‰è´¨é‡æ£€æŸ¥
def pre_clean_quality_check(df):
    print("\n" + "=" * 60)
    print("ğŸ“Š æ•°æ®æ¸…æ´—å‰è´¨é‡æ£€æŸ¥")
    print("=" * 60)

    # ç¼ºå¤±å€¼ç»Ÿè®¡
    missing = df.isnull().sum()
    missing = missing[missing > 0]
    if not missing.empty:
        print("1. ç¼ºå¤±å€¼ç»Ÿè®¡ï¼š")
        for col, cnt in missing.items():
            print(f"   - {col}: {cnt} æ¡ï¼ˆ{cnt / len(df) * 100:.2f}%ï¼‰")
    else:
        print("1. ç¼ºå¤±å€¼ç»Ÿè®¡ï¼šæ— ç¼ºå¤±å€¼")

    # Type 2åˆ—æ— å…³å€¼æå‰æ£€æŸ¥
    if 'Type 2' in df.columns:
        # é¢„å¤„ç†ï¼šå»ç©ºæ ¼å¹¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        df['Type 2'] = df['Type 2'].astype(str).str.strip()
        # ç»Ÿè®¡æ— å…³å€¼æ•°é‡
        invalid_type2 = df[df['Type 2'].isin(INVALID_TYPE2_VALUES)]
        if len(invalid_type2) > 0:
            print(f"2. Type 2åˆ—å«æ— å…³å€¼ï¼ˆ{INVALID_TYPE2_VALUES}ï¼‰çš„è¡Œï¼š{len(invalid_type2)} æ¡ï¼ˆåç»­å°†åˆ é™¤ï¼‰")

    # é‡å¤è¡Œç»Ÿè®¡
    dup_cnt = df.duplicated().sum()
    print(f"3. é‡å¤è¡Œæ•°é‡ï¼š{dup_cnt} æ¡")

    return df


# æ ¸å¿ƒæ•°æ®æ¸…æ´—å‡½æ•°ï¼ˆåŒ…å«Type 2åˆ—æ— å…³å€¼å¤„ç†ï¼‰
def clean_pokemon_data(df):
    print("\n" + "=" * 60)
    print("ğŸ§¹ å¼€å§‹æ‰§è¡Œæ•°æ®æ¸…æ´—")
    print("=" * 60)

    df_clean = df.copy()
    initial_rows = len(df_clean)
    print(f"åˆå§‹æ•°æ®è¡Œæ•°ï¼š{initial_rows}")

    # æ­¥éª¤1ï¼šå¤„ç†Type 2åˆ—æ— å…³å€¼
    print("\n1. Type 2åˆ—æ— å…³å€¼å¤„ç†ï¼š")
    if 'Type 2' in df_clean.columns:
        # é¢„å¤„ç†ï¼šå»ç©ºæ ¼å¹¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼ˆé¿å…ç©ºæ ¼å¯¼è‡´è¯¯åˆ¤ï¼‰
        df_clean['Type 2'] = df_clean['Type 2'].astype(str).str.strip()
        # æ ‡è®°åŒ…å«æ— å…³å€¼çš„è¡Œ
        invalid_type2_mask = df_clean['Type 2'].isin(INVALID_TYPE2_VALUES)
        invalid_type2_cnt = invalid_type2_mask.sum()
        # åˆ é™¤æ— å…³å€¼æ‰€åœ¨è¡Œ
        df_clean = df_clean[~invalid_type2_mask]
        print(f"   - åˆ é™¤Type 2åˆ—å«æ— å…³å€¼ï¼ˆ{INVALID_TYPE2_VALUES}ï¼‰çš„è¡Œï¼š{invalid_type2_cnt} æ¡ï¼Œå‰©ä½™è¡Œæ•°ï¼š{len(df_clean)}")
    else:
        print("   - æ•°æ®ä¸­æ— Type 2åˆ—ï¼Œè·³è¿‡å¤„ç†")

    # æ­¥éª¤2ï¼šé‡å‘½åé¦–åˆ—ä¸ºID
    print("\n2. åˆ—åæ ‡å‡†åŒ–ï¼š")
    if df_clean.columns[0] != 'ID':
        original_first_col = df_clean.columns[0]
        df_clean.rename(columns={original_first_col: 'ID'}, inplace=True)
        print(f"   - é¦–åˆ— '{original_first_col}' é‡å‘½åä¸º 'ID'")
    else:
        print(f"   - é¦–åˆ—å·²ä¸º 'ID'ï¼Œæ— éœ€ä¿®æ”¹")

    # æ­¥éª¤3ï¼šåˆ é™¤æœªå®šä¹‰è¡Œï¼ˆæ’é™¤Type 2åˆ—ç©ºå€¼ï¼Œä½†å…¶æ— å…³å€¼å·²åœ¨æ­¥éª¤1å¤„ç†ï¼‰
    print("\n3. æœªå®šä¹‰è¡Œåˆ é™¤ï¼š")
    undefined_patterns = ['undefined', 'Undefined', 'UNDEFINED', np.nan, '']

    def is_invalid_row(row):
        for col_name, value in row.items():
            if col_name == 'Type 2':
                continue  # Type 2ç©ºå€¼åˆæ³•ï¼Œæ— å…³å€¼å·²åˆ é™¤
            if pd.isna(value) or str(value).strip() in undefined_patterns:
                return True
        return False

    invalid_mask = df_clean.apply(is_invalid_row, axis=1)
    invalid_cnt = invalid_mask.sum()
    df_clean = df_clean[~invalid_mask]
    print(f"   - åˆ é™¤å«æœªå®šä¹‰å€¼çš„è¡Œï¼š{invalid_cnt} æ¡ï¼Œå‰©ä½™è¡Œæ•°ï¼š{len(df_clean)}")

    # æ­¥éª¤4ï¼šåˆ é™¤å®Œå…¨ç©ºè¡Œ
    print("\n4. å®Œå…¨ç©ºè¡Œåˆ é™¤ï¼š")
    empty_row_cnt = df_clean.isnull().all(axis=1).sum()
    df_clean = df_clean.dropna(how='all')
    print(f"   - åˆ é™¤å®Œå…¨ç©ºè¡Œï¼š{empty_row_cnt} æ¡ï¼Œå‰©ä½™è¡Œæ•°ï¼š{len(df_clean)}")

    # æ­¥éª¤5ï¼šæ•°å€¼åˆ—ç±»å‹ä¿®æ­£
    print("\n5. æ•°å€¼åˆ—ç±»å‹ä¿®æ­£ï¼š")
    numeric_columns = ['Total', 'HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def', 'Speed', 'Generation', 'ID']
    existing_numeric_cols = [col for col in numeric_columns if col in df_clean.columns]

    for col in existing_numeric_cols:
        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')
    print(f"   - å·²ä¿®æ­£ {len(existing_numeric_cols)} ä¸ªæ•°å€¼åˆ—ç±»å‹ï¼š{existing_numeric_cols}")

    # æ­¥éª¤6ï¼šç¼ºå¤±å€¼å¡«å……
    print("\n6. ç¼ºå¤±å€¼å¡«å……ï¼š")
    missing_before = df_clean.isnull().sum().sum()

    # æ•°å€¼åˆ—ï¼šä¸­ä½æ•°å¡«å……
    if existing_numeric_cols:
        imputer = SimpleImputer(strategy='median')
        df_clean[existing_numeric_cols] = imputer.fit_transform(df_clean[existing_numeric_cols])
        print(f"   - æ•°å€¼åˆ—ï¼ˆ{existing_numeric_cols}ï¼‰ï¼šä¸­ä½æ•°å¡«å……")

    # åˆ†ç±»å‹åˆ—ï¼ˆé™¤Type 2ï¼‰ï¼šä¼—æ•°å¡«å……
    categorical_cols = ['Type 1', 'Legendary', 'Name']
    existing_cat_cols = [col for col in categorical_cols if col in df_clean.columns]
    for col in existing_cat_cols:
        if df_clean[col].isnull().sum() > 0:
            mode_val = df_clean[col].mode()[0] if len(df_clean[col].mode()) > 0 else 'Unknown'
            df_clean[col] = df_clean[col].fillna(mode_val)
            print(f"   - {col} åˆ—ï¼šä¼—æ•°å¡«å……ï¼ˆå¡«å……å€¼ï¼š{mode_val}ï¼‰")

    # Type 2åˆ—ï¼šç©ºå€¼ä¿ç•™ä¸ºç©ºå­—ç¬¦ä¸²ï¼ˆåˆæ³•ä¸šåŠ¡åœºæ™¯ï¼‰
    if 'Type 2' in df_clean.columns:
        df_clean['Type 2'] = df_clean['Type 2'].fillna('')
        print(f"   - Type 2 åˆ—ï¼šç©ºå€¼ä¿ç•™ä¸ºç©ºå­—ç¬¦ä¸²")

    missing_after = df_clean.isnull().sum().sum()
    print(f"   - å¡«å……å®Œæˆï¼šå¡«å……å‰ {missing_before} ä¸ªç¼ºå¤±å€¼ â†’ å¡«å……å {missing_after} ä¸ªç¼ºå¤±å€¼")

    # æ­¥éª¤7ï¼šåˆ é™¤é‡å¤è¡Œ
    print("\n7. é‡å¤è¡Œå¤„ç†ï¼š")
    duplicates_before = df_clean.duplicated().sum()
    df_clean = df_clean.drop_duplicates()
    duplicates_after = df_clean.duplicated().sum()
    print(f"   - åˆ é™¤é‡å¤è¡Œï¼š{duplicates_before - duplicates_after} æ¡ï¼Œå‰©ä½™é‡å¤è¡Œï¼š{duplicates_after} æ¡")

    # æ­¥éª¤8ï¼šæ•°æ®æ ¼å¼æ ‡å‡†åŒ–
    print("\n8. æ•°æ®æ ¼å¼æ ‡å‡†åŒ–ï¼š")
    # æ–‡æœ¬åˆ—å»ç©ºæ ¼
    text_cols = ['Name', 'Type 1', 'Type 2']
    existing_text_cols = [col for col in text_cols if col in df_clean.columns]
    for col in existing_text_cols:
        df_clean[col] = df_clean[col].astype(str).str.strip()
    print(f"   - æ–‡æœ¬åˆ—å»ç©ºæ ¼ï¼š{existing_text_cols}")

    # Legendaryåˆ—ç»Ÿä¸€ä¸ºTRUE/FALSE
    if 'Legendary' in df_clean.columns:
        true_values = ['TRUE', 'True', 'true', '1', '1.0']
        df_clean['Legendary'] = df_clean['Legendary'].apply(
            lambda x: 'TRUE' if str(x).strip() in true_values else 'FALSE'
        )
        print(f"   - Legendary åˆ—ï¼šç»Ÿä¸€ä¸º TRUE/FALSE æ ¼å¼")

    # æ­¥éª¤9ï¼šåˆ é™¤å®Œå…¨ç©ºåˆ—
    print("\n9. ç©ºåˆ—åˆ é™¤ï¼š")
    empty_cols_before = df_clean.isnull().all(axis=0).sum()
    df_clean = df_clean.loc[:, ~df_clean.isnull().all()]
    empty_cols_after = df_clean.isnull().all(axis=0).sum()
    print(f"   - åˆ é™¤å®Œå…¨ç©ºåˆ—ï¼š{empty_cols_before - empty_cols_after} åˆ—ï¼Œå‰©ä½™åˆ—æ•°ï¼š{len(df_clean.columns)}")

    # æ¸…æ´—ç»“æœæ±‡æ€»
    print("\n" + "=" * 60)
    print("âœ… æ•°æ®æ¸…æ´—å®Œæˆæ€»ç»“")
    print("=" * 60)
    final_rows = len(df_clean)
    print(f"åˆå§‹è¡Œæ•°ï¼š{initial_rows} â†’ æœ€ç»ˆè¡Œæ•°ï¼š{final_rows}")
    print(f"æ€»åˆ é™¤è¡Œæ•°ï¼š{initial_rows - final_rows} æ¡ï¼ˆå«Type 2æ— å…³å€¼è¡Œï¼‰")
    print(f"æœ€ç»ˆæ•°æ®è§„æ¨¡ï¼š{df_clean.shape}")
    print(f"æ•°æ®å®Œæ•´æ€§ï¼š{((1 - df_clean.isnull().sum().sum() / df_clean.size) * 100):.2f}%")
    print(f"é‡å¤è¡Œæ•°é‡ï¼š{df_clean.duplicated().sum()} æ¡")

    return df_clean


# æ¸…æ´—åéªŒè¯ï¼ˆé‡ç‚¹éªŒè¯Type 2åˆ—ï¼‰
def post_clean_validation(df_cleaned, df_original):
    print("\n" + "=" * 60)
    print("ğŸ” æ¸…æ´—åæ•°æ®éªŒè¯")
    print("=" * 60)

    # Type 2åˆ—éªŒè¯
    if 'Type 2' in df_cleaned.columns:
        remaining_type2 = df_cleaned['Type 2'].unique()
        invalid_remaining = [t for t in remaining_type2 if t in INVALID_TYPE2_VALUES]
        if len(invalid_remaining) == 0:
            print(f"1. Type 2åˆ—éªŒè¯ï¼šå·²æˆåŠŸåˆ é™¤æ‰€æœ‰æ— å…³å€¼ï¼ˆ{INVALID_TYPE2_VALUES}ï¼‰")
        else:
            print(f"1. Type 2åˆ—è­¦å‘Šï¼šä»å­˜åœ¨æ— å…³å€¼ {invalid_remaining}")

    # å…¶ä»–åŸºç¡€éªŒè¯
    print("2. åŸºç¡€ä¿¡æ¯ï¼š")
    print(f"   - æ•°æ®è§„æ¨¡ï¼š{df_cleaned.shape}")
    print(f"   - ç¼ºå¤±å€¼ï¼š{df_cleaned.isnull().sum().sum()} ä¸ª")
    print(f"   - é‡å¤è¡Œï¼š{df_cleaned.duplicated().sum()} ä¸ª")


# ä¿å­˜æ¸…æ´—åæ•°æ®
def save_cleaned_data(df_cleaned, filename='pokemon_cleaned.csv'):
    df_cleaned.to_csv(filename, index=False)
    print(f"\nğŸ’¾ æ¸…æ´—åæ•°æ®å·²ä¿å­˜è‡³ï¼š{filename}")


# æ•°æ®æ¸…ç†å»ºè®®
def data_cleaning_recommendations(df, duplicate_rows, outlier_info, special_cases, consistency_issues):
    print("\n" + "=" * 50)
    print("æ•°æ®æ¸…ç†å»ºè®®")
    print("=" * 50)

    recommendations = []

    # 1. é‡å¤æ•°æ®å¤„ç†å»ºè®®
    if not duplicate_rows.empty:
        recommendations.append("å»ºè®®åˆ é™¤å®Œå…¨é‡å¤çš„è¡Œ")

    # 2. å¼‚å¸¸å€¼å¤„ç†å»ºè®®
    for col, outliers in outlier_info.items():
        if not outliers.empty:
            recommendations.append(f"{col}åˆ—ä¸­å­˜åœ¨æ˜æ˜¾å¼‚å¸¸å€¼ï¼Œå»ºè®®æ£€æŸ¥å¹¶ä¿®æ­£")

    # 3. ç‰¹æ®Šå€¼å¤„ç†å»ºè®®
    for case_type, cases in special_cases.items():
        if not cases.empty:
            if 'zero_negative' in case_type:
                col = case_type.replace('_zero_negative', '')
                recommendations.append(f"{col}åˆ—ä¸­å­˜åœ¨0æˆ–è´Ÿå€¼ï¼Œéœ€è¦æ£€æŸ¥")
            elif 'scientific' in case_type:
                col = case_type.replace('_scientific', '')
                recommendations.append(f"{col}åˆ—ä¸­å­˜åœ¨ç§‘å­¦è®¡æ•°æ³•æ•°å€¼ï¼Œå»ºè®®è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼")

    # 4. é€»è¾‘ä¸€è‡´æ€§å»ºè®®
    if 'total_mismatch' in consistency_issues:
        recommendations.append("Totalåˆ—ä¸å„å±æ€§ä¹‹å’Œä¸åŒ¹é…ï¼Œéœ€è¦ä¿®æ­£")
    if 'column_swap' in consistency_issues:
        recommendations.append("Generationå’ŒLegendaryåˆ—å¯èƒ½å­˜åœ¨ç½®æ¢ï¼Œéœ€è¦æ£€æŸ¥")

    # 5. Type 2å¼‚å¸¸å¤„ç†
    type2_col = None
    for col in df.columns:
        if 'Type' in col and '2' in col:
            type2_col = col
            break

    if type2_col:
        # æ£€æŸ¥Type 2ä¸­çš„å¼‚å¸¸å€¼ï¼ˆå¦‚æ•°å­—0ï¼‰
        type2_anomalies = df[df[type2_col].astype(str).str.isdigit()]
        if not type2_anomalies.empty:
            recommendations.append("Type 2åˆ—ä¸­å­˜åœ¨æ•°å€¼ï¼Œå»ºè®®æ¸…ç©ºè¿™äº›å€¼")

    # è¾“å‡ºå»ºè®®
    if recommendations:
        print("å‘ç°ä»¥ä¸‹æ•°æ®è´¨é‡é—®é¢˜ï¼Œå»ºè®®è¿›è¡Œæ¸…ç†:")
        for i, rec in enumerate(recommendations, 1):
            print(f"{i}. {rec}")
    else:
        print("æ•°æ®è´¨é‡è‰¯å¥½ï¼Œæœªå‘ç°æ˜æ˜¾é—®é¢˜")

    return recommendations


# ä¸»å‡½æ•°
def main():
    # åŠ è½½æ•°æ®
    df = load_pokemon_data()

    if df is None:
        print("æ— æ³•è¯»å–æ•°æ®æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„å’Œæ ¼å¼")
        return

    # æ•°æ®æ¦‚è§ˆ
    data_overview(df)

    # æ•°æ®ç±»å‹æ£€æŸ¥
    numeric_cols, categorical_cols = check_data_types(df)

    # é‡å¤å€¼æ£€æµ‹
    duplicate_rows, name_duplicates = detect_duplicates(df)

    # å¼‚å¸¸å€¼æ£€æµ‹
    outlier_info = detect_numeric_outliers(df, numeric_cols)

    # ç‰¹æ®Šå€¼æ£€æµ‹
    special_cases = detect_special_values(df)

    # é€»è¾‘ä¸€è‡´æ€§æ£€æŸ¥
    consistency_issues = check_consistency(df)

    # å¯è§†åŒ–æ£€æµ‹
    visualize_anomalies(df)

    # æ•°æ®æ¸…ç†å»ºè®®
    recommendations = data_cleaning_recommendations(
        df, duplicate_rows, outlier_info, special_cases, consistency_issues
    )

    # æ¸…æ´—å‰è´¨é‡æ£€æŸ¥
    df = pre_clean_quality_check(df)

    # æ‰§è¡Œæ•°æ®æ¸…æ´—
    df_cleaned = clean_pokemon_data(df)

    # æ¸…æ´—åéªŒè¯
    post_clean_validation(df_cleaned, df)

    # ä¿å­˜æ¸…æ´—åæ•°æ®
    save_cleaned_data(df_cleaned)

    print("\n" + "=" * 50)
    print("åˆ†æå’Œæ¸…æ´—æµç¨‹å…¨éƒ¨å®Œæˆï¼")
    print("=" * 50)


if __name__ == "__main__":
    main()