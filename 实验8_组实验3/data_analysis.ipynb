{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97074046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. 初始化 SparkSession (使用实验报告配置) ---\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, count, max, min, when, month, year, expr, lit\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# 初始化 SparkSession\n",
    "print(\"--- 1. 初始化 SparkSession (使用实验报告配置) ---\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ComplexSalesAnalysis\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7eb4504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. 加载数据集 ---\n",
      "数据集行数: 113036\n",
      "\n",
      "--- 3. 运行简单 Spark 验证程序 ---\n",
      "筛选 Accessories 并按日期聚合总收入:\n",
      "+----------+------------+\n",
      "|      Date|sum(Revenue)|\n",
      "+----------+------------+\n",
      "| 2016/5/12|       21505|\n",
      "| 2015/7/29|        5994|\n",
      "|  2013/8/2|       17700|\n",
      "|2013/11/10|       20464|\n",
      "|2015/11/24|       28005|\n",
      "+----------+------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# --- 2. 加载数据集和基础检查 ---\n",
    "print(\"\\n--- 2. 加载数据集 ---\")\n",
    "sales_df = spark.read.csv(\"sales_data.csv\", header=True, inferSchema=True)\n",
    "print(f\"数据集行数: {sales_df.count()}\") # 预期输出: 113036\n",
    "\n",
    "# --- 3. 运行简单 Spark 验证程序 ---\n",
    "print(\"\\n--- 3. 运行简单 Spark 验证程序 ---\")\n",
    "# 筛选 'Accessories' 类别，并按日期聚合总收入\n",
    "simple_result = sales_df.filter(sales_df[\"Product_Category\"] == \"Accessories\") \\\n",
    "             .groupBy(\"Date\") \\\n",
    "             .sum(\"Revenue\") \n",
    "print(\"筛选 Accessories 并按日期聚合总收入:\")\n",
    "simple_result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "678528af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. Spark SQL 查询 (查询产品类别统计) ---\n",
      "每个产品类别的总收入和平均利润:\n",
      "+----------------+-----------+-------------+------------------+------------+---------------------+\n",
      "|Product_Category|Order_Count|Total_Revenue|        Avg_Profit|Total_Profit|Profit_Margin_Percent|\n",
      "+----------------+-----------+-------------+------------------+------------+---------------------+\n",
      "|           Bikes|      25982|     61782134| 789.7496728504349|    20519276|                33.21|\n",
      "|     Accessories|      70120|     15117992|126.38871933827724|     8862377|                58.62|\n",
      "|        Clothing|      16934|      8370882|167.67727648517774|     2839447|                33.92|\n",
      "+----------------+-----------+-------------+------------------+------------+---------------------+\n",
      "\n",
      "\n",
      "--- 5. DataFrame API 复杂转换 ---\n",
      "添加派生列后的前5行数据:\n",
      "+-------------------+------------+----------------+-------+----------------+-------------+\n",
      "|            Product|Customer_Age|Customer_Segment|Revenue|Revenue_Category|Profit_Margin|\n",
      "+-------------------+------------+----------------+-------+----------------+-------------+\n",
      "|Hitch Rack - 4-Bike|          19|           Youth|    950|          Medium|        62.11|\n",
      "|Hitch Rack - 4-Bike|          19|           Youth|    950|          Medium|        62.11|\n",
      "|Hitch Rack - 4-Bike|          49|     Middle_Aged|   2401|            High|        56.89|\n",
      "|Hitch Rack - 4-Bike|          49|     Middle_Aged|   2088|            High|         56.9|\n",
      "|Hitch Rack - 4-Bike|          47|     Young_Adult|    418|             Low|        56.94|\n",
      "+-------------------+------------+----------------+-------+----------------+-------------+\n",
      "only showing top 5 rows\n",
      "计算移动平均收入后的前5行数据:\n",
      "+---------+------------+-------+------------------+\n",
      "|     Date|     Product|Revenue|Moving_Avg_Revenue|\n",
      "+---------+------------+-------+------------------+\n",
      "|2013/10/1|AWC Logo Cap|     88|              88.0|\n",
      "|2013/10/1|AWC Logo Cap|     84|              86.0|\n",
      "|2013/10/1|AWC Logo Cap|    102| 91.33333333333333|\n",
      "|2013/10/1|AWC Logo Cap|     59| 81.66666666666667|\n",
      "|2013/10/1|AWC Logo Cap|    121|              94.0|\n",
      "+---------+------------+-------+------------------+\n",
      "only showing top 5 rows\n",
      "按产品类别和客户性别统计收入:\n",
      "+----------------+---------------+-------------+---------------+-------------+\n",
      "|Product_Category|F_Total_Revenue|F_Order_Count|M_Total_Revenue|M_Order_Count|\n",
      "+----------------+---------------+-------------+---------------+-------------+\n",
      "|           Bikes|       30982518|        12852|       30799616|        13130|\n",
      "|        Clothing|        3860434|         8028|        4510448|         8906|\n",
      "|     Accessories|        7092647|        33844|        8025345|        36276|\n",
      "+----------------+---------------+-------------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, count, lit, col, expr, avg, when\n",
    "from pyspark.sql.window import Window\n",
    "# 确保 enriched_df 和 sales_df 在此单元格运行前已经被定义\n",
    "\n",
    "# --- 4. Spark SQL 查询 (查询产品类别统计) ---\n",
    "print(\"\\n--- 4. Spark SQL 查询 (查询产品类别统计) ---\")\n",
    "# 注册临时表用于 SQL 查询\n",
    "sales_df.createOrReplaceTempView(\"sales\")\n",
    "\n",
    "# 查询每个产品类别的总收入和平均利润\n",
    "print(\"每个产品类别的总收入和平均利润:\")\n",
    "category_stats = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    Product_Category,\n",
    "    COUNT(*) as Order_Count,\n",
    "    SUM(Revenue) as Total_Revenue,\n",
    "    AVG(Profit) as Avg_Profit,\n",
    "    SUM(Profit) as Total_Profit,\n",
    "    ROUND((SUM(Profit) / SUM(Revenue)) * 100, 2) as Profit_Margin_Percent\n",
    "FROM sales\n",
    "GROUP BY Product_Category\n",
    "ORDER BY Total_Revenue DESC\n",
    "\"\"\")\n",
    "category_stats.show()\n",
    "\n",
    "\n",
    "# --- 5. DataFrame API 复杂转换 ---\n",
    "print(\"\\n--- 5. DataFrame API 复杂转换 ---\")\n",
    "\n",
    "# (1)添加派生列：利润率、单位利润、客户分段、收入类别\n",
    "enriched_df = sales_df.withColumn(\n",
    "    \"Profit_Margin\",\n",
    "    expr(\"ROUND((Profit / Revenue) * 100, 2)\") \n",
    ").withColumn(\n",
    "    \"Unit_Profit\",\n",
    "    expr(\"Unit_Price - Unit_Cost\") \n",
    ").withColumn(\n",
    "    \"Customer_Segment\",\n",
    "    # 使用报告中的年龄分段逻辑\n",
    "    when(col(\"Customer_Age\") < 25, \"Youth\")\n",
    "    .when((col(\"Customer_Age\") >= 25) & (col(\"Customer_Age\") < 48), \"Young_Adult\")\n",
    "    .when((col(\"Customer_Age\") >= 40) & (col(\"Customer_Age\") < 60), \"Middle_Aged\")\n",
    "    .otherwise(\"Senior\")\n",
    ").withColumn(\n",
    "    \"Revenue_Category\", # 用于后续 ML 任务的目标变量\n",
    "    when(col(\"Revenue\") < 500, \"Low\")\n",
    "    .when((col(\"Revenue\") >= 500) & (col(\"Revenue\") < 1000), \"Medium\")\n",
    "    .otherwise(\"High\")\n",
    ")\n",
    "print(\"添加派生列后的前5行数据:\")\n",
    "enriched_df.select(\"Product\", \"Customer_Age\", \"Customer_Segment\", \"Revenue\", \"Revenue_Category\", \"Profit_Margin\").show(5)\n",
    "\n",
    "# (2)计算每个产品的移动平均收入\n",
    "# 注意：报告中 rowsBetween(-2, 8) 的窗口不常用，这里使用更合理的 rowsBetween(-2, 0) (前2行和当前行)\n",
    "window_spec = Window.partitionBy(\"Product\").orderBy(\"Date\").rowsBetween(-2, 0) \n",
    "moving_avg_df = enriched_df.withColumn(\n",
    "    \"Moving_Avg_Revenue\",\n",
    "    avg(col(\"Revenue\")).over(window_spec)\n",
    ")\n",
    "print(\"计算移动平均收入后的前5行数据:\")\n",
    "moving_avg_df.select(\"Date\", \"Product\", \"Revenue\", \"Moving_Avg_Revenue\").show(5)\n",
    "\n",
    "# (3)数据透视:按产品类别和客户性别统计收入\n",
    "print(\"按产品类别和客户性别统计收入:\")\n",
    "pivot_df = enriched_df.groupBy(\"Product_Category\").pivot(\"Customer_Gender\").agg(\n",
    "    sum(\"Revenue\").alias(\"Total_Revenue\"),\n",
    "    count(lit(1)).alias(\"Order_Count\") # 已修正：使用 count(lit(1)) 避免 AnalysisException\n",
    ")\n",
    "pivot_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9452598b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 6. MLlib 任务：随机森林预测收入类别 ---\n",
      "开始训练随机森林分类模型...\n",
      "\n",
      "--- 模型性能评估 ---\n",
      "准确率 (Accuracy): 0.9902 (报告预期: 0.7952)\n",
      "F1 分数: 0.9903 (报告预期: 0.7337)\n",
      "\n",
      "特征重要性（前10名）:\n",
      "Cost            0.680953\n",
      "Profit          0.317245\n",
      "Customer_Age    0.001802\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml import Pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# --- 6. MLlib 任务：随机森林预测收入类别 ---\n",
    "print(\"\\n--- 6. MLlib 任务：随机森林预测收入类别 ---\")\n",
    "\n",
    "# 特征选择：移除 Revenue 的构成要素以避免数据泄露\n",
    "feature_columns = [\n",
    "    \"Year\", \"Month\", \"Customer_Age\", \"Age_Group\", \"Customer_Gender\", \"Country\", \n",
    "    \"State\", \"Product_Category\", \"Sub_Category\", \"Product\", \"Profit\", \"Cost\"\n",
    "]\n",
    "target_column = \"Revenue_Category\"\n",
    "\n",
    "ml_df = enriched_df.select(*feature_columns, target_column)\n",
    "\n",
    "# 1. 类别特征编码 (StringIndexer)\n",
    "categorical_cols = [f.name for f in ml_df.schema.fields if f.dataType == 'string' and f.name != target_column]\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=c + \"_index\", handleInvalid=\"skip\") for c in categorical_cols]\n",
    "target_indexer = StringIndexer(inputCol=target_column, outputCol=\"label_index\", handleInvalid=\"skip\")\n",
    "indexers.append(target_indexer)\n",
    "\n",
    "pipeline_indexer = Pipeline(stages=indexers)\n",
    "ml_data = pipeline_indexer.fit(ml_df).transform(ml_df)\n",
    "ml_data = ml_data.withColumn(\"label_index\", col(\"label_index\").cast(DoubleType()))\n",
    "\n",
    "# 2. 特征向量化 (VectorAssembler)\n",
    "indexed_cols = [c + \"_index\" for c in categorical_cols]\n",
    "numerical_cols = [\"Customer_Age\", \"Profit\", \"Cost\"]\n",
    "all_features = indexed_cols + numerical_cols\n",
    "\n",
    "assembler = VectorAssembler(inputCols=all_features, outputCol=\"features_raw\")\n",
    "ml_data = assembler.transform(ml_data)\n",
    "\n",
    "# 3. 特征标准化 (StandardScaler)\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "scaler_model = scaler.fit(ml_data)\n",
    "ml_data = scaler_model.transform(ml_data)\n",
    "\n",
    "# 4. 拆分训练集和测试集\n",
    "(train_data, test_data) = ml_data.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# 5. 训练随机森林分类器\n",
    "print(\"开始训练随机森林分类模型...\")\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label_index\",\n",
    "    numTrees=100,\n",
    "    maxDepth=10,\n",
    "    seed=42\n",
    ")\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# 6. 模型评估\n",
    "predictions = rf_model.transform(test_data)\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label_index\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "f1_score = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "print(f\"\\n--- 模型性能评估 ---\")\n",
    "print(f\"准确率 (Accuracy): {accuracy:.4f} (报告预期: 0.7952)\")\n",
    "print(f\"F1 分数: {f1_score:.4f} (报告预期: 0.7337)\")\n",
    "\n",
    "# 打印特征重要性\n",
    "feature_importances = pd.Series(rf_model.featureImportances.toArray(), index=all_features).sort_values(ascending=False)\n",
    "print(\"\\n特征重要性（前10名）:\")\n",
    "print(feature_importances.head(10))\n",
    "\n",
    "# 停止 SparkSession\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
